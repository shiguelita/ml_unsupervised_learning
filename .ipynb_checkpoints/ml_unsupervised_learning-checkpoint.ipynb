{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Um resumo sobre Modelos de Aprendizagem Não Supervisionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizagem Não Supervisionadas são modelos de Machine Learninig que possuem poucos ou nenhum dado histórico para se basear e conseguir prever os resultados, eles não necessitam de um conjunto de dados que digam quais são as variáveis de saídas corretas, para modelar um algoritmo preditivo. Esses modelos conseguem criar estruturas de dados com base em relações entre as variáveis ou  detectar algumas tendências.\n",
    "\n",
    "A seguir está um breve estudo sobre os alguns destes modelos, sugeridos do Nanodegree in Machine Learning Engineer da Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering(Agrupamento K-means)\n",
    "\n",
    "#### O que é?\n",
    "Um algoritmo que agrupa dados que possuem características similares entre si em um *cluster*. O método K-means tem esse nome porque *K* representa um número, uma vez que é necessário dizer ao algoritmo em quantos clusters deseja a divisão dos dados.\n",
    "K-means é mais utilizade em uso geral,com clusters de tamanhos iguais, geometria plana, sem grandes quantidades de clusters.\n",
    "\n",
    "K-means sempre tenta encontrar clusters circulares, esféricos.\n",
    "\n",
    "Ele usa a medida de distancia para dizer o quão similiar cada observação é, podendo ser:\n",
    "   * Distância Euclidiana: distância euclidiana entre dois pontos em uma linha reta\n",
    "   * Distância de Manhattan:Soma das distâncias entre as linhas que foram 90º entre dois pontos, ou seja, soma da linha y até o ponto de intersecção e a linha x. Essas medidas são sensíveis à diferença de escalas entre variáveis distintas:\n",
    "   * Distância do Cosseno: medida do ângulo entre dois vetores. Se o ângulo é 0 existe total similariade e se é π não existe relação entre os objetos.\n",
    "\n",
    "\n",
    "* **Onde pode ser usado?**\n",
    "    * Performance acadêmica, baseado nas notas de estudantes para separar entre estudantes A, B ou C\n",
    "    * Sistema de diagnóstico\n",
    "    * Classificação de catálogo de filmes\n",
    "    * Classificação de espécies de animais ou plantas\n",
    "    * Segmentação de clientes de acordo com perfis de consumo\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Fácil implentação e interpretação\n",
    "    * Bom quando já se possui uma ideia do número necessários de clusters\n",
    "    * Boa escabilidade com muitas variáveis se o k não for tão alto\n",
    "\n",
    "\n",
    "* **Desvantagens**\n",
    "    * As *sementes (seeds)* inicias tem um grande impacto nos resultados finais devido o local minumm, quando são iniciados em locais não tão adequados. Dificuldade que pode ser contornada rodando o algorítmo muitas vezes\n",
    "    * Difícil de prever o número de clusters (K-Value), pode-se usar *elbow method* para contornar isso\n",
    "    * Sensível a outliers\n",
    "    * Não trbaalha bem com missing values\n",
    "    * Não trabalha bem com clusters que não são circulares ou esféricos\n",
    "    \n",
    "    \n",
    "* **Comandos no Sklearn**     \n",
    "    * n_clusters: número de clusters desejados, padrão 8 \n",
    "    * max_iter: quantas intereções máximas entre atribuir os pontos e mover o centroide, padrão 300\n",
    "    * n_init: quantas vezes o algoritmo muda os centroides para diferentes clusters, padrão 10\n",
    "    \n",
    "    \n",
    "* **Como encontrar o número de clusters (k)**\n",
    "    * **Elbow Method**: método utilizado para  encontrar o número de clusters mais adequados para cada conjunto de dados, ele plota os valores ascendentes de k versus o erro total calculado usando esse k. Abaixo um código para encontrar esse valor:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.clusters import Kmeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X = pd.DataFrame(__)\n",
    "\n",
    "k_values = range(2, len(X)+1, 5)\n",
    "\n",
    "scores = []\n",
    "for k in k_values:\n",
    "    kmeans = Kmeans(n_clusters=k).fit(X)\n",
    "    prediction = kmeans.predict(X)\n",
    "    score = silhouette_score(X, predictions)\n",
    "    scores.append((k, score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering (Agrupamento Hierárquico)\n",
    "\n",
    "#### O que é?\n",
    "O resultado deste algoritmo é uma estrutura de agrupamento que nos proporciona uma indicação visual da relação entre os agrupamentos. Ele uni os dados em clusters de acordo com distâncias mínimas, que são calculadas de acordo com cada método:\n",
    "\n",
    "* **Single Link**: assume que todos os pontos são um cluster e depois os agrupa em clusters de acordo com a distância dos pontos. Depois, quando há um ponto isolado, ele procura os dois pontos mais **próximos** nos dois clusters que é a distância entre os clusters, e e depois une os pontos formando outro cluster. E então escolhemos quantos clusters queremos. **Este método não existe no sklearn**, em vez disso usamos o ***Complete Link***\n",
    "\n",
    "\n",
    "* **Complete Link**: assume que todos os pontos são um cluster e depois os agrupa em clusters de acordo com a distância dos pontos formando outro cluster. E então, quando há um ponto isolado, ele procura os dois pontos mais **afastados** nos dois clusters que é a distância entre os clusters, e e depois une os pontos. E então escolhemos quantos clusters queremos.\n",
    "Para unir outros clusters, ele calcula a distância maior entre os pontos, que vamos nomear de distância X, depois mede a distância X menor entre clusters para uní-los.\n",
    "O problema deste método é que somente um ponto é analisado para verificar a menor distância, sem levar em conta os outros dados, que podem estar unidos de forma mais densa, podendo ser outro cluster.\n",
    "\n",
    "\n",
    "* **Average Link**: verifica a distância entre todos os pontos entre clusters e a média de todas as distâncias é a medida de distância entre os dois clusters.\n",
    "\n",
    "\n",
    "* **Ward's Link**: método que tenta minimizar a variância ao unir os dois clusters. Ele calcula um ponto central entre dois clusters, depois verifica a distância entre cada ponto e o ponto central, os eleva ao quadrado e soma todas as distâncias. Depois ele calcula um ponto central em cada cluster, e subtrai cada distância entre pontos e ponto central de cada cluster, elevada ao quadrado. **Este é o método padrão do sklean.**\n",
    "\n",
    "\n",
    "\n",
    "* **Onde pode ser usado?**\n",
    "    * Usando o método de Average Link, foi possível agrupar proteínas produzidas, usado para criar clusters de tipos de fungos, [documentação](https://www.ncbi.nlm.nih.gov/pubmed/22238666)\n",
    "    * Usando o método Complete Link para desenhar um diagrama de microbioma humano, [documentação](https://www.ncbi.nlm.nih.gov/pubmed/21129376)\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Funciona bem com conjunto de dados dois-crescentes e dois-anéis\n",
    "    * Fácil visualização de cada clusters (dendogramas)\n",
    "    * Especialmente potente quando o conjunto de dados contém relacionamento hierárquico real (EX:biologia evolutiva)\n",
    "    * Representações hierárquicas resultantes são muito informativas\n",
    "\n",
    "\n",
    "* **Desvantagens**\n",
    "    * Necessita escolher qual método mais adequado\n",
    "    * Sensível a ruídos e outilers\n",
    "    * Precisa de capacidade computacional maior quando tiver grandes conjuntos de dados e muitas dimensionalidades\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Clustering - DBSAC (Density Based Spatial Clustering of Application with Noise)\n",
    "\n",
    "#### O que é?\n",
    "O algoritmo agrupa os dados que estão unidos de forma densa, pontos próximos juntos, e nem todos os pontos fazem parte de um cluster, o model nomeia os outros pontos como ruído.\n",
    "Ele verifica a distância ε (epsilon) em volta dos pontos e caso não haja outros pontos próximos suficientes, é considerado ruído. Temos que setar os parametros de número mínimo de pontos por cluster e distância ε.\n",
    "\n",
    "Core point (ponto cetral): é o ponto que possui o número mínimo de pontos requeridos e identifica um cluster. Um mesmo cluster pode ter vários core points\n",
    "Border point (ponto marginal):pontos que não possuem o número mínimo de pontos requeridos em volta, contudo fazem parte de um cluster em volta de um Core Point \n",
    "\n",
    "\n",
    "* **Onde pode ser usado?**\n",
    "    * Analisar tráfego de rede, um administrador de rede consegue separar por grupos os tipos de tráfegos como usuários de downloads massivos e outros. Veja a [documentação](https://conferences.sigcomm.org/sigcomm/2006/papers/minenet-01.pdf)\n",
    "    * Detecção de anomalias em dados de temperatura, na qual os pontos nomeados como ruídos são as anomalias, outliers. Veja a  [documentação](https://ieeexplore.ieee.org/document/5946052)\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Bom com conjunto de dados com ruído ou outliers\n",
    "    * Flexivel para trabalhar com formas e trabanhos diferentes de clusters\n",
    "    * Não é necessário especificar o número de clusters\n",
    "    * Funciona bem com conjunto de dados dois-crescentes e dois-anéis\n",
    "\n",
    "\n",
    "* **Desvantagens**\n",
    "    * Border points que podem ser acessados de dois clusters são atribuídos ao cluster que o encontra primeiro\n",
    "    * Enfrenta dificuldade em encontrar clusters de diferentes densidades, podemos usar HDBSCAN nesses casos\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Çelik, Mete, et al. 2001.[Anomaly detection in temperature data using DBSCAN algorithm](https://ieeexplore.ieee.org/document/5946052)\n",
    "\n",
    "Erman, Jefreey, et al. 2006. [Traffic Classification Using Clustering Algorithms](https://conferences.sigcomm.org/sigcomm/2006/papers/minenet-01.pdf)\n",
    "\n",
    "Portella, Letícia. 2018. [Machine Learning Models - My Cheat List](https://leportella.com/cheatlist/2018/05/20/models-cheat-list.html)\n",
    "\n",
    "Saunders, Diane, et al. 2012. [Using hierarchical clustering of secreted protein families to classify and rank candidate effectors of rust fungi](https://www.ncbi.nlm.nih.gov/pubmed/22238666)\n",
    "\n",
    "Sonagara, Darshan and Badheka, Soham. 2014. [Comparison of Basic Clustering Algorithms](https://pdfs.semanticscholar.org/f0a4/d6bfb37b6c1102f7ef6b0d0f2ef861da6aca.pdf)\n",
    "\n",
    "Spencer, Melanie, et al. 2010. [Association between composition of the human gastrointestinal microbiome and development of fatty liver with choline deficiency](https://www.ncbi.nlm.nih.gov/pubmed/21129376)\n",
    "\n",
    "[Sklearn documentation on Clustering Agglomerative](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "\n",
    "[Sklearn documentation on Clustering K-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "[Sklearn documentation on Clustering DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
