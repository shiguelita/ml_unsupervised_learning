{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Um resumo sobre Modelos de Aprendizagem Não Supervisionada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aprendizagem Não Supervisionadas são modelos de Machine Learninig que possuem poucos ou nenhum dado histórico para se basear e conseguir prever os resultados, eles não necessitam de um conjunto de dados que digam quais são as variáveis de saídas corretas, para modelar um algoritmo preditivo. Esses modelos conseguem criar estruturas de dados com base em relações entre as variáveis ou  detectar algumas tendências.\n",
    "\n",
    "A seguir está um breve estudo sobre os alguns destes modelos, sugeridos do Nanodegree in Machine Learning Engineer da Udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering(Agrupamento K-means)\n",
    "\n",
    "#### O que é?\n",
    "Um algoritmo que agrupa dados que possuem características similares entre si em um *cluster*. O método K-means tem esse nome porque *K* representa um número, uma vez que é necessário dizer ao algoritmo em quantos clusters deseja a divisão dos dados.\n",
    "K-means é mais utilizade em uso geral,com clusters de tamanhos iguais, geometria plana, sem grandes quantidades de clusters.\n",
    "\n",
    "K-means sempre tenta encontrar clusters circulares, esféricos.\n",
    "\n",
    "Ele usa a medida de distancia para dizer o quão similiar cada observação é, podendo ser:\n",
    "   * Distância Euclidiana: distância euclidiana entre dois pontos em uma linha reta\n",
    "   * Distância de Manhattan:Soma das distâncias entre as linhas que foram 90º entre dois pontos, ou seja, soma da linha y até o ponto de intersecção e a linha x. Essas medidas são sensíveis à diferença de escalas entre variáveis distintas:\n",
    "   * Distância do Cosseno: medida do ângulo entre dois vetores. Se o ângulo é 0 existe total similariade e se é π não existe relação entre os objetos.\n",
    "\n",
    "\n",
    "* **Onde pode ser usado?**\n",
    "    * Performance acadêmica, baseado nas notas de estudantes para separar entre estudantes A, B ou C\n",
    "    * Sistema de diagnóstico\n",
    "    * Classificação de catálogo de filmes\n",
    "    * Classificação de espécies de animais ou plantas\n",
    "    * Segmentação de clientes de acordo com perfis de consumo\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Fácil implentação e interpretação\n",
    "    * Bom quando já se possui uma ideia do número necessários de clusters\n",
    "    * Boa escabilidade com muitas variáveis se o k não for tão alto\n",
    "\n",
    "\n",
    "* **Desvantagens**\n",
    "    * As *sementes (seeds)* inicias tem um grande impacto nos resultados finais devido o local minumm, quando são iniciados em locais não tão adequados. Dificuldade que pode ser contornada rodando o algorítmo muitas vezes\n",
    "    * Difícil de prever o número de clusters (K-Value), pode-se usar *elbow method* para contornar isso\n",
    "    * Sensível a outliers\n",
    "    * Não trbaalha bem com missing values\n",
    "    * Não trabalha bem com clusters que não são circulares ou esféricos\n",
    "    \n",
    "    \n",
    "* **Comandos no Sklearn**     \n",
    "    * n_clusters: número de clusters desejados, padrão 8 \n",
    "    * max_iter: quantas intereções máximas entre atribuir os pontos e mover o centroide, padrão 300\n",
    "    * n_init: quantas vezes o algoritmo muda os centroides para diferentes clusters, padrão 10\n",
    "    \n",
    "    \n",
    "* **Como encontrar o número de clusters (k)**\n",
    "    * **Elbow Method**: método utilizado para  encontrar o número de clusters mais adequados para cada conjunto de dados, ele plota os valores ascendentes de k versus o erro total calculado usando esse k. Abaixo um código para encontrar esse valor:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.clusters import Kmeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "X = pd.DataFrame(__)\n",
    "\n",
    "k_values = range(2, len(X)+1, 5)\n",
    "\n",
    "scores = []\n",
    "for k in k_values:\n",
    "    kmeans = Kmeans(n_clusters=k).fit(X)\n",
    "    prediction = kmeans.predict(X)\n",
    "    score = silhouette_score(X, predictions)\n",
    "    scores.append((k, score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering (Agrupamento Hierárquico)\n",
    "\n",
    "#### O que é?\n",
    "O resultado deste algoritmo é uma estrutura de agrupamento que nos proporciona uma indicação visual da relação entre os agrupamentos. Ele uni os dados em clusters de acordo com distâncias mínimas, que são calculadas de acordo com cada método:\n",
    "\n",
    "* **Single Link**: assume que todos os pontos são um cluster e depois os agrupa em clusters de acordo com a distância dos pontos. Depois, quando há um ponto isolado, ele procura os dois pontos mais **próximos** nos dois clusters que é a distância entre os clusters, e e depois une os pontos formando outro cluster. E então escolhemos quantos clusters queremos. **Este método não existe no sklearn**, em vez disso usamos o ***Complete Link***\n",
    "\n",
    "\n",
    "* **Complete Link**: assume que todos os pontos são um cluster e depois os agrupa em clusters de acordo com a distância dos pontos formando outro cluster. E então, quando há um ponto isolado, ele procura os dois pontos mais **afastados** nos dois clusters que é a distância entre os clusters, e e depois une os pontos. E então escolhemos quantos clusters queremos.\n",
    "Para unir outros clusters, ele calcula a distância maior entre os pontos, que vamos nomear de distância X, depois mede a distância X menor entre clusters para uní-los.\n",
    "O problema deste método é que somente um ponto é analisado para verificar a menor distância, sem levar em conta os outros dados, que podem estar unidos de forma mais densa, podendo ser outro cluster.\n",
    "\n",
    "\n",
    "* **Average Link**: verifica a distância entre todos os pontos entre clusters e a média de todas as distâncias é a medida de distância entre os dois clusters.\n",
    "\n",
    "\n",
    "* **Ward's Link**: método que tenta minimizar a variância ao unir os dois clusters. Ele calcula um ponto central entre dois clusters, depois verifica a distância entre cada ponto e o ponto central, os eleva ao quadrado e soma todas as distâncias. Depois ele calcula um ponto central em cada cluster, e subtrai cada distância entre pontos e ponto central de cada cluster, elevada ao quadrado. **Este é o método padrão do sklean.**\n",
    "\n",
    "\n",
    "\n",
    "* **Onde pode ser usado?**\n",
    "    * Usando o método de Average Link, foi possível agrupar proteínas produzidas, usado para criar clusters de tipos de fungos, [documentação](https://www.ncbi.nlm.nih.gov/pubmed/22238666)\n",
    "    * Usando o método Complete Link para desenhar um diagrama de microbioma humano, [documentação](https://www.ncbi.nlm.nih.gov/pubmed/21129376)\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Funciona bem com conjunto de dados dois-crescentes e dois-anéis\n",
    "    * Fácil visualização de cada clusters (dendogramas)\n",
    "    * Especialmente potente quando o conjunto de dados contém relacionamento hierárquico real (EX:biologia evolutiva)\n",
    "    * Representações hierárquicas resultantes são muito informativas\n",
    "\n",
    "\n",
    "* **Desvantagens**\n",
    "    * Necessita escolher qual método mais adequado\n",
    "    * Sensível a ruídos e outilers\n",
    "    * Precisa de capacidade computacional maior quando tiver grandes conjuntos de dados e muitas dimensionalidades\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Clustering - DBSAC (Density Based Spatial Clustering of Application with Noise)\n",
    "\n",
    "#### O que é?\n",
    "O algoritmo agrupa os dados que estão unidos de forma densa, pontos próximos juntos, e nem todos os pontos fazem parte de um cluster, o model nomeia os outros pontos como ruído.\n",
    "Ele verifica a distância ε (epsilon) em volta dos pontos e caso não haja outros pontos próximos suficientes, é considerado ruído. Temos que setar os parametros de número mínimo de pontos por cluster e distância ε.\n",
    "\n",
    "Core point (ponto cetral): é o ponto que possui o número mínimo de pontos requeridos e identifica um cluster. Um mesmo cluster pode ter vários core points\n",
    "Border point (ponto marginal):pontos que não possuem o número mínimo de pontos requeridos em volta, contudo fazem parte de um cluster em volta de um Core Point \n",
    "\n",
    "\n",
    "* **Onde pode ser usado?**\n",
    "    * Analisar tráfego de rede, um administrador de rede consegue separar por grupos os tipos de tráfegos como usuários de downloads massivos e outros. Veja a [documentação](https://conferences.sigcomm.org/sigcomm/2006/papers/minenet-01.pdf)\n",
    "    * Detecção de anomalias em dados de temperatura, na qual os pontos nomeados como ruídos são as anomalias, outliers. Veja a  [documentação](https://ieeexplore.ieee.org/document/5946052)\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Bom com conjunto de dados com ruído ou outliers\n",
    "    * Flexivel para trabalhar com formas e trabanhos diferentes de clusters\n",
    "    * Não é necessário especificar o número de clusters\n",
    "    * Funciona bem com conjunto de dados dois-crescentes e dois-anéis\n",
    "\n",
    "\n",
    "* **Desvantagens**\n",
    "    * Border points que podem ser acessados de dois clusters são atribuídos ao cluster que o encontra primeiro\n",
    "    * Enfrenta dificuldade em encontrar clusters de diferentes densidades, podemos usar HDBSCAN nesses casos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models - GMM (Modelos de Mistura de Gaussianas)\n",
    "\n",
    "#### O que é?\n",
    "O algoritmo de agrupamento leve na qual todo dado é parte de todos os clusters, contudo com níveis diferentes de pertencimento. Ele faz uma distribuição de dados, e se for gaussiana eles tem um nivel de agrupamento maior, ou seja, ele mistura as distribuições gaussianas para separar os clusters.\n",
    "\n",
    "Ele funciona pelo algortimo de Maximização da Expectativa, nos seguintes passos:\n",
    "\n",
    "* Definir K, número distribuições gaussianas. Setar média e variâncias \n",
    "  \n",
    "* Passo E, agrupamento suave dos dados. calcula o grau de pertenciomento de cada ponto para cada cluster\n",
    "  \n",
    "* Passo M, passo da maximização, usar os dados do passo anterior e criar novos paramettos de média e variância para cada cluster. de forma ponderada levando em conta o nível de pertencimetno de cada ponto\n",
    "  \n",
    "* Validar o log-likelihood (log-verossimilhança) para checar se converge. Quanto maior o número mais certeza temos que o modelo de mistura que geramos é responsavel pela criação dos dados ou que encaixe no dataset que temos\n",
    "\n",
    "\n",
    "* **Onde pode ser usado?**\n",
    "    * Leitura de dados de sensores para encontrar rotinas de pessoas. Ex: Como um sensor de deslocamento, o algoritmo consegue identificar quando estão no escritório, jantando ou fora. Ou também, consegue diferenciar qual o meio de condução que a pessoa utilizada. Veja a [documentação](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.3152&rep=rep1&type=pdf)\n",
    "    * Identificar fala, assinaturas, biometria.\n",
    "    * Visão computacional, consegue detectar fundos de imagem, por exemplo tirando pessoas de uma vídeo e deixando apenas o cenário. Veja a [documentação](http://www.ai.mit.edu/projects/vsam/Publications/stauffer_cvpr98_track.pdf)\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Precisa de menos treino que outros modelos, como regressão logística\n",
    "    * Alta escalabilidade\n",
    "    * Não é sensível a *features* com pouca importância\n",
    "    * Retorna o grau de certeza da resposta\n",
    "    * Agrupamento leve, que faz com que amostras possam fazer parte de múltiplos clusters\n",
    "\n",
    "\n",
    "* **Desvantagens**\n",
    "    * Sensível aos valores de inicialização\n",
    "    * A taxa de convergência é lenta\n",
    "    * Não é bom em aprender com intereções de *features*, por exemplo, se vocês gosta de sorvete de flocos e chocolate, mas odeia os dois sabores juntos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passos de análise para modelos de aprendizagem não supervisionadas\n",
    "   \n",
    "* Processar os dados e escolher as melhores *features*, variáveis de entrada, via Principal Component Analysis (PCA)\n",
    "* Escolher o algorítmo de agrupamento de acordo com os dados e tunar ele\n",
    "* Validar o agrupamento\n",
    "* Interprestações dos resultados, quais os insights\n",
    "\n",
    "### Feature Scaling (dimensionamento de recurso)\n",
    "\n",
    "Método para transformar as features com intervalos semelhantes para que possam ser compáveis, normalmente entre 0 e 1.\n",
    "\n",
    "No Sklearn existe um método que faz isso, o ***MinMaxScaler***, que transforma os dados nessa escala, contundo precisa lembrar que seu array tem que ter números *float* (decimais)\n",
    "\n",
    "\n",
    "Exemplos de modelos que são afetados por Feature Scaling:\n",
    "* K-Means\n",
    "* SVM com RBF\n",
    "\n",
    "\n",
    "Exemplos de modelos que **não** são afetados por Feature Scaling:\n",
    "* Regressão Lienar\n",
    "* Decision Trees\n",
    "\n",
    "\n",
    "\n",
    "* **Vantagens**\n",
    "    * Ter um número confiável em relação ao que você espera na saída\n",
    "    \n",
    "    \n",
    "* **Desvantagens**\n",
    "    * Sensível a outilers\n",
    "    \n",
    "    \n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "* Usado para reduzir a dimensionalidade dos *features*\n",
    "* PCA é um modelo sistematizado de transformar as *features* de entradas em componentes principais.\n",
    "* Esses Componentes Principais ficam disponíveis para se usar no lugar das *features* originais\n",
    "* Pode-se rankear os Componentes Principais\n",
    "* Os Componentes Principais devem ser perpendiculares entre si (90º), que significa que são independentes entre si\n",
    "* O número máximo de Componentes Principais que se pode obter é igual ao número de inputs do conjunto de dados, contudo é recomendável não usar todos, e sim os que estiverem com maior raking\n",
    "\n",
    "**Variáveis Mensuravéis**: pode medir de maneira direta, como m², número de quartos, ranking escolar\n",
    "\n",
    "\n",
    "**Variáveis Latentes**: não podemos medir diretamente, mas é representada por outros indicadores (as mensuráveis), gerando um sentido teórico. Ex: tamanho é a variável latente e suas variáveis mensuráveis são m² e número de quartos.\n",
    "\n",
    "* **Quando usar?**\n",
    "    * Quando queremos ver se variáveis latentes que pode ser mostradas nos padrões dos dados\n",
    "    * Diminuir a dimensionalidade para:\n",
    "        * Visualizar quando temos grandes dimensionaldiades\n",
    "        * Diminuir os ruídos   \n",
    "        * Fazer um pré processamento dos dados\n",
    "\n",
    "\n",
    "### Validação de modelos de agrupamento\n",
    "\n",
    "* **Índices Externos**: usado quando os dados originais estiverem rotulados.Um dos seus índices é o Adjusted Rand score, sendo de -1 a 1, quanto maior melhor.\n",
    "\n",
    "\n",
    "* **Índices Internos**: usado para pedir o ajusto entre os dados e a estrutura. Um dos seus índices é o Silhouette score, sendo de -1 a 1, quanto maior melhor, não é um bom índice para DBSCAN, nem tão pouco para agrupamentos definidos como dois-anéis. Bom para agrupamentos compactos, densos, circulares. Para DBSCAN é bom utilizar o índice DBCV.\n",
    "\n",
    "\n",
    "* **Índices Relativos**: indica qual estrutura de clusters é melhor em algum sentido\n",
    "\n",
    "\n",
    "** Os índices de validação são definidos como:**\n",
    "   * **Compactação**: medida de proximidade dos elementos em cada grupo\n",
    "   * **Separabilidade**: medida de quão distantes os clusters estão uns dos outos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referências\n",
    "\n",
    "Çelik, Mete, et al. 2001.[Anomaly detection in temperature data using DBSCAN algorithm](https://ieeexplore.ieee.org/document/5946052)\n",
    "\n",
    "Erman, Jefreey, et al. 2006. [Traffic Classification Using Clustering Algorithms](https://conferences.sigcomm.org/sigcomm/2006/papers/minenet-01.pdf)\n",
    "\n",
    "Portella, Letícia. 2018. [Machine Learning Models - My Cheat List](https://leportella.com/cheatlist/2018/05/20/models-cheat-list.html)\n",
    "\n",
    "Saunders, Diane, et al. 2012. [Using hierarchical clustering of secreted protein families to classify and rank candidate effectors of rust fungi](https://www.ncbi.nlm.nih.gov/pubmed/22238666)\n",
    "\n",
    "Sonagara, Darshan and Badheka, Soham. 2014. [Comparison of Basic Clustering Algorithms](https://pdfs.semanticscholar.org/f0a4/d6bfb37b6c1102f7ef6b0d0f2ef861da6aca.pdf)\n",
    "\n",
    "Spencer, Melanie, et al. 2010. [Association between composition of the human gastrointestinal microbiome and development of fatty liver with choline deficiency](https://www.ncbi.nlm.nih.gov/pubmed/21129376)\n",
    "\n",
    "Stauffer, Chris and Grmson W, E, L. [Adaptive background mixture models for real-time tracking](http://www.ai.mit.edu/projects/vsam/Publications/stauffer_cvpr98_track.pdf)\n",
    "\n",
    "Sun, Feng-Tso, et al. [Nonparametric Discovery of Human Routines from Sensor Data](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.681.3152&rep=rep1&type=pdf)\n",
    "\n",
    "[Sklearn documentation on Clustering Agglomerative](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "\n",
    "[Sklearn documentation on Clustering K-means](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "[Sklearn documentation on Clustering DBSCAN](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
